{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      },
      "source": [
        "# Finetune Llama-3 with LLaMA Factory\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr7rB3szzhtx"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giM74oK1rRIH",
        "outputId": "8af5e5bc-6ca3-49d2-907f-8945598eaa2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdata\u001b[0m/        \u001b[01;34mexamples\u001b[0m/  MANIFEST.in     README_zh.md      \u001b[01;34mscripts\u001b[0m/  \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mcache\u001b[0m/        \u001b[01;34mdocker\u001b[0m/      LICENSE    pyproject.toml  requirements.txt  setup.py\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile   README.md       \u001b[01;34msaves\u001b[0m/            \u001b[01;34msrc\u001b[0m/\n",
            "Obtaining file:///content/drive/MyDrive/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<=4.43.4,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (4.42.4)\n",
            "Collecting datasets<=2.20.0,>=2.16.0 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting accelerate<=0.32.0,>=0.30.1 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading accelerate-0.32.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting peft<=0.12.0,>=0.11.1 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting gradio>=4.0.0 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading gradio-4.41.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.1.99)\n",
            "Collecting tiktoken (from llamafactory==0.8.4.dev0)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.20.3)\n",
            "Collecting uvicorn (from llamafactory==0.8.4.dev0)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.8.2)\n",
            "Collecting fastapi (from llamafactory==0.8.4.dev0)\n",
            "  Downloading fastapi-0.112.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.8.4.dev0)\n",
            "  Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.7.1)\n",
            "Collecting fire (from llamafactory==0.8.4.dev0)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (1.26.4)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.3.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.32.0,>=0.30.1->llamafactory==0.8.4.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.32.0,>=0.30.1->llamafactory==0.8.4.dev0) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.32.0,>=0.30.1->llamafactory==0.8.4.dev0) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (4.66.5)\n",
            "Collecting xxhash (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (3.10.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.7.1)\n",
            "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (9.4.0)\n",
            "Collecting pydub (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading ruff-0.5.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.0.7)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.4.dev0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.4.dev0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.4.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.4.dev0) (2.20.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.43.4,>=4.41.2->llamafactory==0.8.4.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.43.4,>=4.41.2->llamafactory==0.8.4.dev0) (0.19.1)\n",
            "Collecting tyro>=0.5.11 (from trl<=0.9.6,>=0.8.6->llamafactory==0.8.4.dev0)\n",
            "  Downloading tyro-0.8.6-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.4.dev0) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->llamafactory==0.8.4.dev0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.4.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.4.dev0) (2.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.4.dev0) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=2.20.0,>=2.16.0->llamafactory==0.8.4.dev0) (3.3.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (13.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.8.4.dev0) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.8.4.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.8.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.1.2)\n",
            "Downloading accelerate-0.32.0-py3-none-any.whl (314 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.41.0-py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m93.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.112.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.5.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.6-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Building wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.8.4.dev0-0.editable-py3-none-any.whl size=21041 sha256=193cd02dbe58e69ca50ceaea692bb1f62f41e16349a2ae0a49b294e2a958fb97\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o4w5v4s0/wheels/7d/56/99/47917bcdf5276670e628f88bc94e50f75d796be2730c20db58\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=3a5d79a922ac77c0c806fcfc7d3143ff38e09419b3cb2e1a8414d1ac9482245c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: pydub, xxhash, websockets, tomlkit, shtab, semantic-version, ruff, python-multipart, pyarrow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, h11, fsspec, fire, ffmpy, dill, aiofiles, uvicorn, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, tyro, sse-starlette, nvidia-cusolver-cu12, httpx, fastapi, gradio-client, datasets, gradio, bitsandbytes, accelerate, trl, peft, llamafactory\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.0\n",
            "    Uninstalling tomlkit-0.13.0:\n",
            "      Successfully uninstalled tomlkit-0.13.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.32.1\n",
            "    Uninstalling accelerate-0.32.1:\n",
            "      Successfully uninstalled accelerate-0.32.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.32.0 aiofiles-23.2.1 bitsandbytes-0.43.3 datasets-2.20.0 dill-0.3.8 fastapi-0.112.0 ffmpy-0.4.0 fire-0.6.0 fsspec-2024.5.0 gradio-4.41.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llamafactory-0.8.4.dev0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 orjson-3.10.7 peft-0.12.0 pyarrow-17.0.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.5.7 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.3 starlette-0.37.2 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.6 tyro-0.8.6 uvicorn-0.30.6 websockets-12.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# %cd /content/\n",
        "# %rm -rf LLaMA-Factory\n",
        "# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd /content/drive/MyDrive/LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUZ2SiRD1eTV",
        "outputId": "eff5152a-3517-414e-f551-fffc446cc874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPFZ8qo71y5z",
        "outputId": "fa9ac5af-21ab-4d77-adb2-1eae09cba90c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RXn_YQnn9f"
      },
      "source": [
        "### Check GPU environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMWs6nBR1sTi",
        "outputId": "2141881a-dfed-4c23-e76c-12c3b778bdec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge-chinese\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese) (1.16.0)\n",
            "Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: rouge-chinese\n",
            "Successfully installed rouge-chinese-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge-chinese"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydMMcx1T2j08",
        "outputId": "f8bfb410-ab57-4d94-c348-4a66695a282c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 4.42.4\n",
            "Uninstalling transformers-4.42.4:\n",
            "  Successfully uninstalled transformers-4.42.4\n",
            "Collecting transformers==4.43.1\n",
            "  Downloading transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m629.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.1) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.1) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.1) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.1) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.1) (0.4.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.1) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.1) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.43.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.43.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.43.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.43.1) (2024.7.4)\n",
            "Downloading transformers-4.43.1-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "Successfully installed transformers-4.43.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall transformers -y && pip install transformers==4.43.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeYs5Lz-QJYk"
      },
      "source": [
        "## Update Identity Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap_fvMBsQHJc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Llama-3\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      },
      "source": [
        "## Fine-tune model via LLaMA Board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLsdS6V5yUMy",
        "outputId": "e4c0f5aa-be1f-46ee-da77-984ca700e0a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/LLaMA-Factory\n",
            "2024-08-14 00:02:28.340078: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-08-14 00:02:28.357663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-14 00:02:28.378687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-14 00:02:28.385136: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-14 00:02:28.400221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-14 00:02:29.668116: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://c690299c135c165adf.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "2024-08-14 00:03:45.452069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-14 00:03:45.473417: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-14 00:03:45.479928: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-14 00:03:46.721858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/14/2024 00:03:52 - WARNING - llamafactory.hparams.parser - Evaluating model in 4/8-bit mode may cause lower scores.\n",
            "08/14/2024 00:03:52 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "tokenizer_config.json: 100% 55.4k/55.4k [00:00<00:00, 4.82MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 24.6MB/s]\n",
            "special_tokens_map.json: 100% 296/296 [00:00<00:00, 2.66MB/s]\n",
            "[INFO|tokenization_utils_base.py:2289] 2024-08-14 00:03:53,797 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/8c22764a7e3675c50d4c7c9a4edb474456022b16/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2289] 2024-08-14 00:03:53,797 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2289] 2024-08-14 00:03:53,797 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/8c22764a7e3675c50d4c7c9a4edb474456022b16/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2289] 2024-08-14 00:03:53,797 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/8c22764a7e3675c50d4c7c9a4edb474456022b16/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2533] 2024-08-14 00:03:54,163 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "08/14/2024 00:03:54 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "08/14/2024 00:03:54 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>\n",
            "08/14/2024 00:03:54 - INFO - llamafactory.data.loader - Loading dataset enhanced_test_data_0813.json...\n",
            "Generating train split: 400 examples [00:00, 1478.35 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 400/400 [00:00<00:00, 1774.31 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 400/400 [00:04<00:00, 98.60 examples/s] \n",
            "eval example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 2170, 264, 3488, 36864, 8479, 11, 701, 3465, 374, 311, 7168, 422, 459, 23495, 8893, 706, 264, 11388, 481, 29962, 2865, 11, 374, 520, 5326, 315, 834, 833, 4210, 505, 2512, 11, 477, 690, 387, 5675, 311, 1833, 5352, 369, 810, 1109, 220, 1591, 2919, 13, 578, 20212, 1121, 649, 387, 16287, 1534, 1139, 459, 2612, 11914, 304, 279, 2768, 3645, 25, 1115, 23495, 8893, 690, 617, 510, 13066, 14655, 3529, 13478, 481, 14, 16476, 60, 29962, 2865, 11, 374, 510, 54228, 36317, 14617, 60, 311, 387, 5675, 311, 1833, 5352, 369, 927, 220, 1591, 2919, 11, 323, 374, 520, 510, 12156, 39579, 59768, 14, 10516, 34200, 14, 16476, 60, 5326, 315, 834, 833, 4210, 505, 2512, 3196, 389, 872, 2860, 2919, 5675, 311, 1833, 5352, 323, 279, 29736, 315, 3221, 9860, 29528, 13, 1442, 279, 23495, 8893, 706, 912, 5326, 315, 834, 833, 4210, 505, 2512, 11, 1243, 3060, 29962, 2865, 1288, 387, 56089, 477, 279, 8893, 374, 11000, 311, 387, 5675, 311, 1833, 5352, 13, 2057, 1505, 6029, 304, 279, 17649, 9002, 1268, 279, 5300, 7482, 527, 5552, 311, 5675, 311, 1833, 5352, 320, 43, 11042, 52, 8, 2704, 304, 23495, 6978, 11, 499, 649, 13488, 2768, 7482, 323, 872, 5536, 389, 8893, 38231, 304, 2512, 25, 6898, 556, 33542, 85, 37478, 40143, 320, 3065, 8, 8266, 25, 8483, 706, 6982, 430, 13263, 20392, 304, 34979, 374, 10815, 311, 2731, 38231, 304, 2512, 13, 44430, 889, 527, 22815, 14691, 34979, 527, 2753, 4461, 311, 834, 85839, 13, 11325, 19, 4605, 25, 19241, 13519, 430, 4827, 11325, 19, 14921, 320, 68, 1326, 2637, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 8, 527, 5938, 449, 66281, 2890, 20124, 323, 1253, 3063, 311, 5190, 7969, 315, 445, 11042, 52, 11, 5423, 304, 6978, 449, 11084, 8624, 18094, 13, 19545, 4078, 25, 578, 955, 315, 4034, 320, 68, 1326, 2637, 31969, 37563, 5623, 3820, 709, 11, 78196, 19545, 8, 649, 10383, 38231, 13, 29900, 13847, 21728, 8541, 311, 7417, 71628, 323, 8108, 445, 11042, 52, 13, 83627, 8266, 25, 60747, 519, 3278, 5496, 449, 23495, 1253, 617, 2204, 20392, 12912, 304, 2512, 13, 4427, 7978, 4284, 430, 20209, 649, 3060, 62425, 7931, 311, 4822, 17045, 477, 1391, 49895, 872, 5845, 311, 9604, 37256, 13, 31180, 39168, 535, 31969, 8266, 25, 44430, 889, 656, 539, 1212, 31180, 71123, 6514, 1253, 3217, 5190, 7969, 315, 93144, 19025, 11, 902, 649, 3063, 311, 834, 833, 3531, 505, 2512, 13, 40312, 22891, 25, 35321, 40312, 18094, 320, 68, 1326, 2637, 6566, 220, 17, 11, 76949, 12104, 13795, 8, 1253, 81584, 449, 7319, 445, 11042, 52, 4245, 311, 279, 84140, 315, 18646, 13803, 323, 6514, 13, 9734, 278, 9069, 8266, 25, 68502, 315, 29962, 2865, 16967, 477, 1579, 29962, 2865, 649, 13519, 8009, 6514, 71628, 11, 902, 374, 3629, 10815, 311, 7319, 445, 11042, 52, 13, 16923, 323, 39700, 8266, 25, 8560, 83664, 323, 4785, 4814, 649, 5536, 2890, 20124, 323, 6514, 71628, 11, 13893, 6522, 311, 445, 11042, 52, 13, 2947, 2223, 8266, 323, 12517, 28780, 25, 9983, 1862, 14726, 323, 3070, 9293, 2704, 649, 10383, 264, 8893, 596, 5845, 311, 10519, 5912, 18985, 21728, 13, 8589, 10260, 25, 578, 24790, 315, 264, 8893, 439, 264, 3389, 34756, 449, 11084, 8624, 649, 13519, 264, 5190, 5326, 315, 445, 11042, 52, 4245, 311, 810, 6485, 2890, 3966, 91173, 527, 1401, 5789, 304, 279, 17649, 430, 2686, 1268, 5370, 7482, 29243, 311, 279, 5326, 315, 834, 833, 4210, 505, 2512, 369, 23495, 6978, 25, 6898, 556, 33542, 85, 37478, 40143, 320, 3065, 8, 8266, 25, 19241, 1501, 430, 6978, 889, 527, 21356, 17045, 304, 34979, 527, 2753, 4461, 311, 834, 85839, 505, 2512, 13, 362, 37538, 3477, 16717, 430, 71628, 311, 34979, 374, 16996, 369, 20958, 38231, 304, 2512, 323, 18189, 279, 5326, 315, 445, 11042, 52, 320, 48353, 311, 11359, 47197, 570, 11325, 19, 4605, 25, 8483, 706, 21091, 430, 4827, 11325, 19, 14921, 527, 5938, 449, 5190, 7969, 315, 834, 833, 3531, 505, 2512, 13, 44430, 449, 11084, 33119, 437, 455, 4099, 1253, 3217, 810, 2890, 36505, 430, 7958, 872, 5845, 311, 9604, 37256, 320, 68, 1326, 2637, 64574, 402, 2382, 1880, 453, 2637, 220, 679, 15, 570, 19545, 4078, 25, 578, 7138, 315, 21728, 320, 94685, 19579, 7120, 27742, 8, 34453, 38231, 13, 29900, 398, 13847, 21728, 527, 10815, 449, 13241, 71628, 323, 4827, 445, 11042, 52, 7969, 320, 68, 1326, 2637, 473, 16960, 1880, 453, 2637, 220, 679, 17, 570, 83627, 8266, 25, 60747, 519, 3278, 449, 23495, 1253, 3663, 5016, 11774, 304, 2512, 20392, 13, 4427, 17649, 13533, 430, 20209, 649, 62425, 8738, 2512, 11, 1418, 3885, 13519, 433, 649, 1391, 49895, 28116, 4245, 311, 2890, 4819, 477, 3674, 9547, 320, 68, 1326, 2637, 423, 27253, 1880, 453, 2637, 220, 679, 22, 570, 31180, 39168, 535, 31969, 8266, 25, 578, 6996, 315, 61568, 389, 31180, 71123, 6514, 706, 1027, 49393, 449, 7319, 834, 833, 3531, 505, 23495, 2512, 13, 44430, 889, 656, 539, 16988, 304, 16195, 2512, 1253, 2733, 43206, 323, 28520, 6068, 704, 320, 68, 1326, 2637, 15130, 5771, 9700, 1880, 453, 2637, 220, 679, 21, 570, 40312, 22891, 25, 35321, 40312, 18094, 13519, 7191, 8624, 31020, 11, 902, 706, 1027, 5938, 449, 7319, 5326, 315, 834, 833, 3531, 13, 44430, 449, 810, 5199, 2890, 4819, 1253, 1505, 433, 810, 5107, 311, 10519, 5912, 2512, 320, 68, 1326, 2637, 735, 6713, 7215, 1880, 453, 2637, 220, 679, 17, 570, 9734, 278, 9069, 8266, 25, 19241, 13519, 430, 6996, 315, 29962, 2865, 16967, 374, 264, 5326, 8331, 369, 834, 833, 3531, 13, 44430, 889, 527, 539, 15870, 41223, 1253, 2733, 2753, 8599, 311, 872, 2512, 323, 810, 4461, 311, 6068, 704, 320, 68, 1326, 2637, 100138, 3520, 1880, 453, 2637, 220, 679, 20, 570, 16923, 323, 39700, 8266, 25, 8560, 83664, 706, 1027, 11054, 439, 264, 5326, 8331, 369, 445, 11042, 52, 13, 44430, 889, 527, 539, 1664, 5392, 414, 3384, 1253, 3217, 810, 2890, 11774, 430, 69033, 872, 5845, 311, 9604, 37256, 320, 68, 1326, 2637, 1226, 12329, 1880, 453, 2637, 220, 1049, 24, 570, 2947, 2223, 8266, 323, 12517, 28780, 25, 9983, 1862, 706, 1027, 6982, 311, 1514, 264, 9200, 3560, 304, 38231, 13, 44430, 889, 6996, 3674, 1862, 11, 3629, 16717, 555, 60439, 2704, 11, 1253, 387, 520, 5190, 5326, 369, 834, 833, 3531, 505, 2512, 320, 68, 1326, 2637, 1443, 28342, 2319, 72, 1880, 453, 2637, 220, 679, 24, 570, 8589, 10260, 25, 36931, 3118, 388, 323, 1884, 449, 11084, 8624, 527, 520, 7319, 5326, 369, 445, 11042, 52, 13, 19241, 4284, 430, 1521, 7931, 1253, 3663, 810, 5199, 30740, 311, 2512, 11, 6522, 311, 834, 833, 3531, 320, 68, 1326, 2637, 473, 16499, 1880, 453, 2637, 220, 1049, 23, 570, 8586, 374, 264, 12399, 315, 1268, 5370, 7482, 29243, 311, 279, 5326, 315, 29962, 2865, 20124, 304, 23495, 6978, 11, 3196, 389, 6484, 17649, 25, 6898, 556, 33542, 85, 37478, 40143, 320, 3065, 8, 8266, 25, 2467, 52461, 311, 34979, 374, 6089, 49393, 449, 32145, 29962, 46735, 13, 19241, 21356, 1501, 430, 6978, 889, 527, 36051, 306, 311, 872, 34979, 68128, 617, 4827, 29962, 21577, 320, 68, 1326, 2637, 7281, 1293, 1880, 453, 2637, 220, 1049, 15, 26, 468, 467, 7881, 1880, 453, 2637, 220, 679, 16, 570, 80480, 84, 1324, 477, 834, 833, 3531, 505, 34979, 3063, 311, 459, 7319, 5326, 315, 11388, 481, 29962, 2865, 13, 11325, 19, 4605, 25, 12310, 11325, 19, 14921, 527, 5938, 449, 5190, 29962, 21577, 13, 8483, 15151, 430, 6978, 449, 11084, 33119, 437, 455, 4099, 320, 10516, 11325, 19, 14921, 8, 527, 2753, 4461, 311, 11322, 29962, 46735, 320, 68, 1326, 2637, 735, 6853, 1964, 1880, 453, 2637, 220, 1049, 16, 570, 12310, 11325, 19, 5990, 649, 1101, 13519, 810, 15748, 8624, 11, 902, 69226, 988, 6514, 27375, 13, 19545, 4078, 25, 29900, 21728, 320, 94685, 8, 527, 5938, 449, 2731, 16967, 323, 6373, 315, 29962, 2865, 13, 362, 4007, 1766, 430, 6978, 889, 9604, 5912, 1833, 5352, 37256, 527, 810, 4461, 311, 11322, 323, 10519, 29962, 46735, 320, 68, 1326, 2637, 473, 16960, 1880, 453, 2637, 220, 679, 17, 570, 83627, 8266, 25, 83627, 649, 10383, 29962, 2865, 20124, 13, 19241, 617, 6982, 430, 20895, 3278, 889, 10519, 71628, 311, 34979, 649, 11322, 29962, 46735, 11, 719, 1884, 889, 656, 539, 527, 520, 5326, 315, 3515, 5190, 29962, 21577, 11, 902, 649, 7958, 2225, 50150, 323, 63746, 2890, 320, 68, 1326, 2637, 386, 1073, 34237, 1880, 453, 2637, 220, 679, 17, 570, 31180, 39168, 535, 31969, 8266, 25, 16065, 7246, 315, 31180, 71123, 6514, 706, 1027, 5938, 449, 13241, 2890, 20124, 304, 23495, 6978, 11, 2737, 2731, 29962, 2865, 2585, 13, 578, 1080, 55885, 315, 31180, 323, 23495, 649, 3063, 311, 2731, 71628, 311, 34979, 323, 8617, 4827, 29962, 21577, 320, 68, 1326, 2637, 2175, 30827, 1880, 453, 2637, 220, 679, 20, 570, 40312, 22891, 25, 578, 40312, 14830, 48862, 1887, 15151, 8624, 33824, 13, 44430, 304, 3010, 18094, 3629, 617, 5190, 29962, 21577, 11, 439, 11084, 8624, 97303, 449, 66281, 22852, 734, 323, 6514, 2077, 320, 68, 1326, 2637, 507, 62561, 1880, 453, 2637, 220, 679, 15, 570, 9734, 278, 9069, 51803, 25, 29900, 29962, 2865, 16967, 374, 16996, 369, 18646, 23495, 6514, 13, 19241, 1501, 430, 6978, 889, 527, 41223, 14134, 527, 810, 4461, 311, 49553, 311, 872, 34979, 323, 11322, 29962, 46735, 320, 68, 1326, 2637, 100138, 3520, 1880, 453, 2637, 220, 679, 20, 570, 16923, 323, 39700, 8266, 25, 8560, 83664, 323, 3428, 2547, 4785, 649, 48291, 5536, 22852, 734, 323, 6514, 71628, 11, 6522, 311, 5190, 29962, 21577, 320, 68, 1326, 2637, 1226, 12329, 1880, 453, 2637, 220, 1049, 24, 570, 45773, 43226, 2704, 374, 5938, 449, 11293, 41265, 315, 34979, 13, 2947, 2223, 8266, 323, 12517, 28780, 25, 9983, 1862, 6067, 11, 3629, 16717, 555, 60439, 2704, 11, 649, 10383, 6514, 71628, 323, 2890, 20124, 13, 362, 6996, 315, 3674, 1862, 1253, 1121, 304, 66281, 71628, 311, 34979, 11, 6522, 311, 5190, 29962, 21577, 320, 68, 1326, 2637, 1443, 28342, 2319, 72, 1880, 453, 2637, 220, 679, 24, 570, 8589, 10260, 25, 44430, 21771, 439, 3389, 3118, 388, 449, 11084, 8624, 3629, 617, 5190, 29962, 21577, 4245, 311, 23540, 6514, 61568, 13, 19241, 1501, 430, 1521, 7931, 527, 520, 264, 5190, 5326, 369, 8009, 6514, 20124, 320, 68, 1326, 2637, 473, 16499, 1880, 453, 2637, 220, 1049, 23, 4390, 37692, 13235, 22293, 25, 578, 23495, 8893, 374, 264, 220, 868, 1060, 6418, 29738, 815, 27742, 37256, 25, 220, 679, 23, 12, 2437, 12, 508, 11, 679, 23, 12, 2839, 12, 1691, 11, 679, 23, 12, 2371, 12, 914, 11, 679, 23, 12, 2304, 12, 1419, 11, 679, 23, 12, 2705, 12, 1544, 11, 679, 23, 12, 2589, 12, 914, 11, 679, 23, 12, 2318, 12, 1682, 11, 679, 23, 12, 2545, 12, 1627, 11, 679, 23, 12, 717, 12, 508, 11, 679, 24, 12, 2437, 12, 1591, 11, 679, 24, 12, 2839, 12, 1591, 11, 679, 24, 12, 2371, 12, 1187, 11, 679, 24, 12, 2304, 12, 1682, 11, 679, 24, 12, 2705, 12, 1627, 11, 679, 24, 12, 2589, 12, 1187, 11, 679, 24, 12, 2318, 12, 2148, 11, 679, 24, 12, 2545, 12, 1691, 11, 679, 24, 12, 605, 12, 777, 11, 679, 24, 12, 806, 12, 1419, 11, 679, 24, 12, 717, 12, 1591, 11, 2366, 15, 12, 1721, 12, 914, 11, 2366, 15, 12, 2437, 12, 1313, 11, 2366, 15, 12, 2589, 12, 2437, 11, 2366, 15, 12, 2589, 12, 508, 11, 2366, 15, 12, 2318, 12, 508, 11, 2366, 15, 12, 2545, 12, 1313, 11, 2366, 15, 12, 605, 12, 1313, 11, 2366, 15, 12, 806, 12, 1187, 11, 2366, 15, 12, 717, 12, 1187, 11, 2366, 16, 12, 1721, 12, 1691, 11, 2366, 16, 12, 2437, 12, 914, 11, 2366, 16, 12, 2839, 12, 914, 11, 2366, 16, 12, 2371, 12, 1591, 11, 2366, 16, 12, 2304, 12, 1591, 11, 2366, 16, 12, 2705, 12, 1591, 11, 2366, 16, 12, 2589, 12, 1682, 11, 2366, 16, 12, 2318, 12, 1627, 11, 2366, 16, 12, 2545, 12, 1591, 11, 2366, 16, 12, 605, 12, 1591, 11, 2366, 16, 12, 806, 12, 914, 11, 2366, 16, 12, 717, 12, 1419, 14472, 940, 21728, 25, 220, 679, 23, 12, 1721, 12, 1419, 11, 679, 23, 12, 2437, 12, 1691, 11, 679, 23, 12, 2839, 12, 1691, 11, 679, 23, 12, 2371, 12, 914, 11, 679, 23, 12, 2304, 12, 1419, 11, 679, 23, 12, 2705, 12, 1544, 11, 679, 23, 12, 2589, 12, 914, 11, 679, 23, 12, 2318, 12, 1682, 11, 679, 23, 12, 806, 12, 1627, 11, 679, 24, 12, 1721, 12, 1682, 11, 679, 24, 12, 2437, 12, 1591, 11, 679, 24, 12, 2839, 12, 1591, 11, 679, 24, 12, 2371, 12, 1187, 11, 679, 24, 12, 2304, 12, 1187, 11, 679, 24, 12, 2705, 12, 1627, 11, 679, 24, 12, 2589, 12, 1544, 11, 679, 24, 12, 2318, 12, 2148, 11, 679, 24, 12, 2545, 12, 1691, 11, 679, 24, 12, 605, 12, 777, 11, 679, 24, 12, 806, 12, 1419, 11, 679, 24, 12, 717, 12, 1187, 11, 2366, 15, 12, 1721, 12, 914, 11, 2366, 15, 12, 2304, 12, 508, 11, 2366, 15, 12, 2705, 12, 508, 11, 2366, 15, 12, 2589, 12, 1691, 11, 2366, 15, 12, 2318, 12, 508, 11, 2366, 15, 12, 2545, 12, 1313, 11, 2366, 15, 12, 605, 12, 1313, 11, 2366, 15, 12, 806, 12, 1627, 11, 2366, 15, 12, 717, 12, 1591, 11, 2366, 16, 12, 1721, 12, 1691, 11, 2366, 16, 12, 2437, 12, 914, 11, 2366, 16, 12, 2839, 12, 914, 11, 2366, 16, 12, 2371, 12, 1591, 11, 2366, 16, 12, 2304, 12, 1591, 11, 2366, 16, 12, 2705, 12, 1591, 11, 2366, 16, 12, 2589, 12, 1682, 11, 2366, 16, 12, 2318, 12, 1591, 11, 2366, 16, 12, 2545, 12, 966, 11, 2366, 16, 12, 605, 12, 1591, 11, 2366, 16, 12, 806, 12, 914, 32282, 315, 2919, 358, 942, 28073, 307, 71123, 15419, 320, 32260, 27344, 8, 32031, 389, 1855, 4034, 2457, 25, 220, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 11, 966, 37097, 2919, 5675, 311, 1833, 709, 25, 220, 15, 11829, 8893, 6898, 556, 33542, 85, 37478, 40143, 320, 3065, 8, 892, 2533, 23842, 315, 23495, 25, 220, 7322, 19, 11, 8899, 18, 11, 7994, 16, 11, 8874, 21, 11, 6804, 19, 11, 5894, 24, 11, 9423, 22, 11, 10148, 17, 11, 10465, 16, 11, 9756, 20, 11, 9992, 20, 11, 11286, 18, 11, 10718, 15, 11, 10513, 15, 11, 11515, 18, 11, 8258, 19, 11, 11908, 24, 11, 10967, 15, 11, 11256, 23, 11, 10828, 18, 11, 9741, 19, 11, 9367, 21, 11, 1049, 17, 11, 9639, 18, 11, 11056, 19, 11, 12652, 19, 11, 11227, 22, 11, 12112, 22, 11, 13762, 17, 11, 9716, 19, 11, 10697, 23, 11, 14261, 18, 11, 12245, 16, 11, 11727, 20, 11, 14590, 20, 11, 8273, 21, 11, 14052, 22, 11, 14205, 22, 11, 5154, 15, 11, 12326, 23, 11, 3192, 21, 11, 15966, 19, 11829, 8893, 4752, 1027, 304, 34979, 369, 25, 220, 7322, 19, 11, 8899, 18, 11, 7994, 16, 11, 8874, 21, 11, 6804, 19, 11, 5894, 24, 11, 9423, 22, 11, 10148, 17, 11, 10465, 16, 11, 9756, 20, 11, 9992, 20, 11, 11286, 18, 11, 10718, 15, 11, 10513, 15, 11, 11515, 18, 11, 8258, 19, 11, 11908, 24, 11, 10967, 15, 11, 11256, 23, 11, 10828, 18, 11, 9741, 19, 11, 9367, 21, 11, 1049, 17, 11, 9639, 18, 11, 11056, 19, 11, 12652, 19, 11, 11227, 22, 11, 12112, 22, 11, 13762, 17, 11, 9716, 19, 11, 10697, 23, 11, 14261, 18, 11, 12245, 16, 11, 11727, 20, 11, 14590, 20, 11, 8273, 21, 11, 14052, 22, 11, 14205, 22, 11, 5154, 15, 11, 12326, 23, 11, 3192, 21, 11, 15966, 19, 885, 36381, 357, 2784, 82, 25, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 44139, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 11, 16511, 49871, 6395, 53, 732, 35, 19, 2704, 25, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 11, 11325, 19, 2237, 374, 220, 15, 2849, 824, 41999, 2606, 55336, 1087, 40381, 519, 2704, 25, 20895, 11, 20895, 11, 20895, 11, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 20895, 11, 20895, 11, 20895, 11, 20895, 11, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 11, 539, 20895, 844, 33, 350, 2898, 2704, 25, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 1212, 1701, 220, 18, 6748, 31180, 71123, 6514, 11, 1212, 1701, 2006, 39, 320, 40, 942, 28073, 307, 8, 323, 46573, 320, 40, 942, 28073, 307, 39168, 535, 40143, 705, 1212, 1701, 2006, 39, 320, 40, 942, 28073, 307, 8, 323, 46573, 320, 40, 942, 28073, 307, 39168, 535, 40143, 705, 1212, 1701, 2006, 39, 320, 40, 942, 28073, 307, 8, 323, 46573, 320, 40, 942, 28073, 307, 39168, 535, 40143, 705, 1212, 1701, 2006, 39, 320, 40, 942, 28073, 307, 8, 323, 46573, 320, 40, 942, 28073, 307, 39168, 535, 40143, 705, 1212, 1701, 2006, 39, 320, 40, 942, 28073, 307, 8, 323, 46573, 320, 40, 942, 28073, 307, 39168, 535, 40143, 705, 4686, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 11, 539, 1212, 1701, 31180, 71123, 6514, 1196, 24755, 6566, 25, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 21317, 11, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 21317, 11, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 21317, 11, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 705, 40312, 6566, 374, 6566, 220, 16, 320, 300, 1631, 418, 13795, 570, 53, 37478, 2865, 25, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 11388, 481, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 11388, 481, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 56089, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 56089, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 11, 9987, 74891, 25, 220, 220, 2287, 11, 1227, 11, 2397, 11, 1227, 11, 5538, 11, 1227, 11, 2397, 11, 2397, 11, 2397, 11, 5538, 11, 5495, 11, 5547, 11, 5538, 11, 5495, 11, 5495, 11, 1399, 11, 5538, 11, 5538, 11, 5538, 11, 5538, 11, 5538, 11, 5538, 10856, 2287, 10856, 2287, 11, 2614, 11, 2031, 11, 2031, 11, 3080, 11, 3080, 11, 2287, 11, 2287, 11, 2397, 11, 5538, 11, 1227, 11, 1227, 10856, 1227, 11, 2397, 11, 1227, 11, 2397, 11, 5547, 11, 5495, 11, 1399, 76080, 12764, 955, 25, 220, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 11, 6401, 5955, 13, 27312, 955, 25, 220, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 31969, 37563, 5623, 3820, 709, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 31969, 37563, 5623, 3820, 709, 11, 31969, 37563, 5623, 3820, 709, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 31969, 37563, 5623, 3820, 709, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11, 78196, 19545, 520, 420, 28913, 11978, 59239, 540, 894, 25, 220, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 21317, 11, 21317, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 21317, 11, 21317, 11, 21317, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 21317, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 11, 36931, 51968, 7586, 349, 15864, 289, 14, 36557, 31974, 320, 6620, 19, 27, 1049, 5738, 1844, 29092, 3016, 84791, 2223, 2704, 25, 220, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 1006, 5285, 9293, 2704, 25, 220, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 3467, 2089, 2006, 64856, 11, 3467, 2089, 2006, 64856, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 21317, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 21317, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 11, 4276, 63151, 2112, 332, 17462, 2704, 25, 220, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 21317, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 21317, 11, 2876, 8811, 77, 414, 3384, 11, 21317, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 21317, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 11, 2876, 8811, 77, 414, 3384, 13, 128009, 128006, 78191, 128007, 271]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "As a question answering agent, your task is to predict if an HIV patient has a detectable viral load, is at risk of disengaging from care, or will be lost to follow-up for more than 28 days. The prediction result can be formalized into an output sentence in the following format: This HIV patient will have [suppressed/detectable/unknown] viral load, is [possibly/unlikely] to be lost to follow-up for over 28 days, and is at [high/moderate/low/no/unknown] risk of disengaging from care based on their total days lost to follow-up and the likelihood of near-term mortality. If the HIV patient has no risk of disengaging from care, then either viral load should be suppressed or the patient is possibly to be lost to follow-up. To find evidence in the literature regarding how the specified variables are related to lost to follow-up (LTFU) status in HIV patients, you can explore following variables and their impact on patient retention in care: Antiretroviral Therapy (ART) Status: Research has shown that consistent engagement in ART is linked to better retention in care. Patients who are actively continuing ART are less likely to disengage. CD4 Count: Studies indicate that lower CD4 counts (e.g., CD4 level is 0 cell per cubic millimeters) are associated with poorer health outcomes and may lead to higher rates of LTFU, especially in patients with advanced disease stages. Visit Type: The type of visit (e.g., Treatment supporter drug pick up, Scheduled Visit) can influence retention. Regular scheduled visits tend to improve adherence and reduce LTFU. Pregnancy Status: Pregnant women living with HIV may have different engagement patterns in care. Some studies suggest that pregnancy can either motivate individuals to stay engaged or complicate their ability to attend appointments. TB Preventive Treatment Status: Patients who do not start TB preventive treatment may experience higher rates of morbidity, which can lead to disengagement from care. WHO Stage: Higher WHO stages (e.g., stage 2, mildly symptomatic) may correlate with increased LTFU due to the complexities of managing symptoms and treatment. Viral Load Status: Lack of viral load monitoring or high viral load can indicate poor treatment adherence, which is often linked to increased LTFU. Weight and Nutrition Status: Malnutrition and weight loss can impact health outcomes and treatment adherence, potentially leading to LTFU. Marital Status and Family Planning: Social support structures and family planning status can influence a patient's ability to maintain regular healthcare visits. Client Category: The classification of a patient as a late presenter with advanced disease can indicate a higher risk of LTFU due to more complex health needs.Here are key areas in the literature that address how various variables relate to the risk of disengaging from care for HIV patients: Antiretroviral Therapy (ART) Status: Studies show that patients who are consistently engaged in ART are less likely to disengage from care. A systematic review indicated that adherence to ART is crucial for maintaining retention in care and reducing the risk of LTFU (Lost to Follow-Up). CD4 Count: Research has demonstrated that lower CD4 counts are associated with higher rates of disengagement from care. Patients with advanced immunosuppression may experience more health complications that affect their ability to attend appointments (e.g., Mugavero et al., 2010). Visit Type: The nature of visits (scheduled versus unscheduled) influences retention. Regularly scheduled visits are linked with improved adherence and lower LTFU rates (e.g., Higa et al., 2012). Pregnancy Status: Pregnant women with HIV may face unique challenges in care engagement. Some literature suggests that pregnancy can motivate continued care, while others indicate it can complicate attendance due to health issues or social factors (e.g., Dyer et al., 2017). TB Preventive Treatment Status: The lack of initiation on TB preventive treatment has been correlated with increased disengagement from HIV care. Patients who do not engage in comprehensive care may feel overwhelmed and subsequently drop out (e.g., Muñoz et al., 2016). WHO Stage: Higher WHO stages indicate greater disease severity, which has been associated with increased risk of disengagement. Patients with more significant health issues may find it more difficult to maintain regular care (e.g., Kranzer et al., 2012). Viral Load Status: Studies indicate that lack of viral load monitoring is a risk factor for disengagement. Patients who are not regularly monitored may feel less connected to their care and more likely to drop out (e.g., Kearney et al., 2015). Weight and Nutrition Status: Malnutrition has been identified as a risk factor for LTFU. Patients who are not well-nourished may experience more health challenges that inhibit their ability to attend appointments (e.g., Weiser et al., 2009). Marital Status and Family Planning: Social support has been shown to play a critical role in retention. Patients who lack social support, often indicated by marital status, may be at higher risk for disengagement from care (e.g., Shokoohi et al., 2019). Client Category: Late presenters and those with advanced disease are at increased risk for LTFU. Studies suggest that these individuals may face more significant barriers to care, leading to disengagement (e.g., Hogg et al., 2008).Here is a summary of how various variables relate to the risk of viral load outcomes in HIV patients, based on existing literature: Antiretroviral Therapy (ART) Status: Adherence to ART is directly correlated with achieving viral suppression. Studies consistently show that patients who are adherent to their ART regimen have lower viral loads (e.g., Paterson et al., 2000; Wainberg et al., 2011). Interruptions or disengagement from ART lead to an increased risk of detectable viral load. CD4 Count: Low CD4 counts are associated with higher viral loads. Research indicates that patients with advanced immunosuppression (low CD4 counts) are less likely to achieve viral suppression (e.g., Kelleher et al., 2001). Low CD4 levels can also indicate more severe disease, which complicates treatment effectiveness. Visit Type: Regular visits (scheduled) are associated with better monitoring and management of viral load. A study found that patients who attend regular follow-up appointments are more likely to achieve and maintain viral suppression (e.g., Higa et al., 2012). Pregnancy Status: Pregnancy can influence viral load outcomes. Studies have shown that pregnant women who maintain adherence to ART can achieve viral suppression, but those who do not are at risk of having higher viral loads, which can affect both maternal and fetal health (e.g., Mofenson et al., 2012). TB Preventive Treatment Status: Initiation of TB preventive treatment has been associated with improved health outcomes in HIV patients, including better viral load control. The co-management of TB and HIV can lead to better adherence to ART and thus lower viral loads (e.g., Getahun et al., 2015). WHO Stage: The WHO clinical staging system indicates disease progression. Patients in later stages often have higher viral loads, as advanced disease correlates with poorer immune function and treatment response (e.g., O'Brien et al., 2010). Viral Load Monitoring: Regular viral load monitoring is crucial for managing HIV treatment. Studies show that patients who are monitored frequently are more likely to adhere to their ART and achieve viral suppression (e.g., Kearney et al., 2015). Weight and Nutrition Status: Malnutrition and low body weight can negatively impact immune function and treatment adherence, leading to higher viral loads (e.g., Weiser et al., 2009). Poor nutritional status is associated with reduced efficacy of ART. Marital Status and Family Planning: Social support systems, often indicated by marital status, can influence treatment adherence and health outcomes. A lack of social support may result in poorer adherence to ART, leading to higher viral loads (e.g., Shokoohi et al., 2019). Client Category: Patients classified as late presenters with advanced disease often have higher viral loads due to delayed treatment initiation. Studies show that these individuals are at a higher risk for poor treatment outcomes (e.g., Hogg et al., 2008).\n",
            "Patient Medical Records: The HIV patient is a 15 year-old Female.Scheduled appointments: 2018-02-20,2018-03-21,2018-04-25,2018-05-23,2018-06-27,2018-07-25,2018-08-29,2018-09-26,2018-12-20,2019-02-28,2019-03-28,2019-04-24,2019-05-29,2019-06-26,2019-07-24,2019-08-31,2019-09-21,2019-10-19,2019-11-23,2019-12-28,2020-01-25,2020-02-22,2020-07-02,2020-07-20,2020-08-20,2020-09-22,2020-10-22,2020-11-24,2020-12-24,2021-01-21,2021-02-25,2021-03-25,2021-04-28,2021-05-28,2021-06-28,2021-07-29,2021-08-26,2021-09-28,2021-10-28,2021-11-25,2021-12-23.Actual visits: 2018-01-23,2018-02-21,2018-03-21,2018-04-25,2018-05-23,2018-06-27,2018-07-25,2018-08-29,2018-11-26,2019-01-29,2019-02-28,2019-03-28,2019-04-24,2019-05-24,2019-06-26,2019-07-27,2019-08-31,2019-09-21,2019-10-19,2019-11-23,2019-12-24,2020-01-25,2020-05-20,2020-06-20,2020-07-21,2020-08-20,2020-09-22,2020-10-22,2020-11-26,2020-12-28,2021-01-21,2021-02-25,2021-03-25,2021-04-28,2021-05-28,2021-06-28,2021-07-29,2021-08-28,2021-09-30,2021-10-28,2021-11-25.Number of days Isoniazid preventive therapy (TB prevention) prescribed on each visit date: 30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30.Total days lost to follow up: 0.The patient Antiretroviral Therapy (ART) time since diagnosis of HIV: 1154,1183,1211,1246,1274,1309,1337,1372,1461,1525,1555,1583,1610,1640,1673,1704,1739,1760,1788,1823,1854,1886,2002,2033,2064,2094,2127,2157,2192,2224,2248,2283,2311,2345,2375,2406,2437,2467,2500,2528,2556,2584.The patienthas been in ART for: 1154,1183,1211,1246,1274,1309,1337,1372,1461,1525,1555,1583,1610,1640,1673,1704,1739,1760,1788,1823,1854,1886,2002,2033,2064,2094,2127,2157,2192,2224,2248,2283,2311,2345,2375,2406,2437,2467,2500,2528,2556,2584.ARV stauts: CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CHANGE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV, CONTINUE ARV.CD4 status: CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters, CD4 level is 0 cell per cubic millimeters.Pregnant status: pregnant, pregnant, pregnant, pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, pregnant, pregnant, pregnant, pregnant, pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant, not pregnant.TB TPT status: not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, start using 3HP TB preventive treatment, start using INH (Isoniazid) and IPT (Isoniazid Preventive Therapy), start using INH (Isoniazid) and IPT (Isoniazid Preventive Therapy), start using INH (Isoniazid) and IPT (Isoniazid Preventive Therapy), start using INH (Isoniazid) and IPT (Isoniazid Preventive Therapy), start using INH (Isoniazid) and IPT (Isoniazid Preventive Therapy), complete TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment, not start using TB preventive treatment.WHO stage: WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), NA, WHO stage is stage 1 (asymptomatic), NA, WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), NA, WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic), WHO stage is stage 1 (asymptomatic).Viral load: unknown, unknown, unknown, unknown, unknown, detectable, unknown, unknown, unknown, unknown, unknown, unknown, unknown, unknown, unknown, unknown, detectable, unknown, unknown, unknown, unknown, unknown, unknown, unknown, unknown, suppressed, unknown, unknown, unknown, unknown, unknown, unknown, unknown, unknown, unknown, unknown, unknown, suppressed, unknown, unknown, unknown, unknown, unknown, unknown, unknown.Weight:  66,64,65,64,62,64,65,65,65,62,63,61,62,63,63,60,62,62,62,62,62,62,,66,,66,68,70,70,67,67,66,66,65,62,64,64,,64,65,64,65,61,63,60.Health facility type:  Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center, Health Center.Visit type:  Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Treatment supporter drug pick up, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Treatment supporter drug pick up, Treatment supporter drug pick up, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Treatment supporter drug pick up, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic, Scheduled Visit at this clinic.Client cateogry:  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, NA, NA, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, NA, NA, NA, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, NA, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client, Late Presentation/Late presentation w/Advanced Disease (CD4<200)/Unstable client.Marital status:  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA.Family planning status:  NA, NA, NA, NA, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, DEPO INJECTION, DEPO INJECTION, NA, NA, NA, NA, NA, NA, NA, NA, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NA, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING, NOT USING.Nutrition status:  Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, NA, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, NA, Not malnourished, NA, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, NA, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished, Not malnourished.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "config.json: 100% 855/855 [00:00<00:00, 6.88MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-08-14 00:04:00,660 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/8c22764a7e3675c50d4c7c9a4edb474456022b16/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-14 00:04:00,661 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.43.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "08/14/2024 00:04:00 - WARNING - llamafactory.model.model_utils.rope - Input length is smaller than max length. Consider increase input length.\n",
            "08/14/2024 00:04:00 - INFO - llamafactory.model.model_utils.rope - Using linear scaling strategy and setting scaling factor to 1.0\n",
            "08/14/2024 00:04:00 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.\n",
            "08/14/2024 00:04:00 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 116MB/s]\n",
            "[INFO|modeling_utils.py:3621] 2024-08-14 00:04:00,933 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/8c22764a7e3675c50d4c7c9a4edb474456022b16/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/4 [00:00<?, ?it/s]\n",
            "model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   0% 10.5M/4.98G [00:00<00:52, 95.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 41.9M/4.98G [00:00<00:24, 205MB/s] \u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 73.4M/4.98G [00:00<00:20, 237MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 105M/4.98G [00:00<00:19, 252MB/s] \u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 136M/4.98G [00:00<00:18, 263MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 168M/4.98G [00:00<00:17, 268MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 199M/4.98G [00:00<00:17, 272MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 231M/4.98G [00:00<00:17, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 262M/4.98G [00:01<00:17, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 294M/4.98G [00:01<00:16, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 325M/4.98G [00:01<00:16, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 357M/4.98G [00:01<00:16, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 388M/4.98G [00:01<00:16, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 419M/4.98G [00:01<00:16, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 451M/4.98G [00:01<00:16, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 482M/4.98G [00:01<00:16, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 514M/4.98G [00:01<00:16, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 545M/4.98G [00:02<00:15, 279MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 577M/4.98G [00:02<00:15, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 608M/4.98G [00:02<00:15, 279MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 640M/4.98G [00:02<00:15, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 671M/4.98G [00:02<00:15, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 703M/4.98G [00:02<00:15, 272MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 734M/4.98G [00:02<00:15, 272MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 765M/4.98G [00:02<00:15, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 797M/4.98G [00:02<00:15, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 828M/4.98G [00:03<00:14, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 860M/4.98G [00:03<00:14, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 891M/4.98G [00:03<00:14, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 923M/4.98G [00:03<00:14, 279MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 954M/4.98G [00:03<00:14, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 986M/4.98G [00:03<00:14, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 1.02G/4.98G [00:03<00:14, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.05G/4.98G [00:03<00:14, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.08G/4.98G [00:03<00:14, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.11G/4.98G [00:04<00:13, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.14G/4.98G [00:04<00:13, 280MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.17G/4.98G [00:04<00:13, 282MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.21G/4.98G [00:04<00:13, 282MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.24G/4.98G [00:04<00:13, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.27G/4.98G [00:04<00:13, 282MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.30G/4.98G [00:04<00:13, 279MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.33G/4.98G [00:04<00:13, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.36G/4.98G [00:04<00:12, 280MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.39G/4.98G [00:05<00:12, 279MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.43G/4.98G [00:05<00:12, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.46G/4.98G [00:05<00:12, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.49G/4.98G [00:05<00:12, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.52G/4.98G [00:05<00:12, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.55G/4.98G [00:05<00:12, 280MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.58G/4.98G [00:05<00:12, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.61G/4.98G [00:05<00:12, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.65G/4.98G [00:05<00:12, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.68G/4.98G [00:06<00:11, 279MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.71G/4.98G [00:06<00:11, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.74G/4.98G [00:06<00:11, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.77G/4.98G [00:06<00:11, 280MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.80G/4.98G [00:06<00:11, 279MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.84G/4.98G [00:06<00:11, 279MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.87G/4.98G [00:06<00:11, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.90G/4.98G [00:06<00:11, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.93G/4.98G [00:07<00:11, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.96G/4.98G [00:07<00:10, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 1.99G/4.98G [00:07<00:10, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.02G/4.98G [00:07<00:10, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.06G/4.98G [00:07<00:10, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.09G/4.98G [00:07<00:10, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.12G/4.98G [00:07<00:10, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.15G/4.98G [00:07<00:10, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.18G/4.98G [00:07<00:10, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.21G/4.98G [00:08<00:10, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.24G/4.98G [00:08<00:09, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.28G/4.98G [00:08<00:09, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.31G/4.98G [00:08<00:09, 274MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.34G/4.98G [00:08<00:09, 274MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.37G/4.98G [00:08<00:09, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.40G/4.98G [00:08<00:09, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.43G/4.98G [00:08<00:09, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.46G/4.98G [00:08<00:09, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.50G/4.98G [00:09<00:09, 274MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.53G/4.98G [00:09<00:08, 273MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.56G/4.98G [00:09<00:08, 274MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.59G/4.98G [00:09<00:08, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.62G/4.98G [00:09<00:08, 272MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.65G/4.98G [00:09<00:08, 270MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.68G/4.98G [00:09<00:08, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.72G/4.98G [00:09<00:08, 274MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.75G/4.98G [00:09<00:08, 272MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.78G/4.98G [00:10<00:08, 274MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.81G/4.98G [00:10<00:07, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.84G/4.98G [00:10<00:07, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.87G/4.98G [00:10<00:07, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.90G/4.98G [00:10<00:07, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.94G/4.98G [00:10<00:07, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 2.97G/4.98G [00:10<00:07, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 3.00G/4.98G [00:10<00:07, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.03G/4.98G [00:11<00:07, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.06G/4.98G [00:11<00:07, 273MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.09G/4.98G [00:11<00:06, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.12G/4.98G [00:11<00:06, 274MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.16G/4.98G [00:11<00:06, 272MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.19G/4.98G [00:11<00:06, 274MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.22G/4.98G [00:11<00:06, 272MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.25G/4.98G [00:11<00:06, 274MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.28G/4.98G [00:11<00:06, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.31G/4.98G [00:12<00:06, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.34G/4.98G [00:12<00:07, 230MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.38G/4.98G [00:12<00:06, 242MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.41G/4.98G [00:12<00:06, 252MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.44G/4.98G [00:12<00:05, 261MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.47G/4.98G [00:12<00:05, 267MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.50G/4.98G [00:12<00:05, 273MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.53G/4.98G [00:12<00:05, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.57G/4.98G [00:13<00:05, 280MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.60G/4.98G [00:13<00:04, 280MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.63G/4.98G [00:13<00:04, 280MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.66G/4.98G [00:13<00:04, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.69G/4.98G [00:13<00:04, 285MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.72G/4.98G [00:13<00:04, 282MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.75G/4.98G [00:13<00:04, 282MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  76% 3.79G/4.98G [00:13<00:04, 283MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  77% 3.82G/4.98G [00:13<00:04, 283MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  77% 3.85G/4.98G [00:14<00:03, 282MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.88G/4.98G [00:14<00:03, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.91G/4.98G [00:14<00:03, 280MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.94G/4.98G [00:14<00:03, 283MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 3.97G/4.98G [00:14<00:03, 284MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 4.01G/4.98G [00:14<00:03, 283MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  81% 4.04G/4.98G [00:14<00:03, 282MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.07G/4.98G [00:14<00:03, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.10G/4.98G [00:14<00:03, 280MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  83% 4.13G/4.98G [00:15<00:03, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.16G/4.98G [00:15<00:02, 283MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.19G/4.98G [00:15<00:02, 281MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  85% 4.23G/4.98G [00:15<00:02, 283MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.26G/4.98G [00:15<00:02, 280MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.29G/4.98G [00:15<00:02, 275MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.32G/4.98G [00:15<00:02, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.35G/4.98G [00:15<00:02, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.38G/4.98G [00:15<00:02, 276MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  89% 4.41G/4.98G [00:16<00:02, 268MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  89% 4.45G/4.98G [00:16<00:01, 268MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  90% 4.48G/4.98G [00:16<00:01, 270MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.51G/4.98G [00:16<00:01, 273MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.54G/4.98G [00:16<00:01, 273MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.57G/4.98G [00:16<00:01, 273MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.60G/4.98G [00:16<00:01, 273MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  93% 4.63G/4.98G [00:16<00:01, 274MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.67G/4.98G [00:16<00:01, 279MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.70G/4.98G [00:17<00:01, 277MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  95% 4.73G/4.98G [00:17<00:00, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.76G/4.98G [00:17<00:00, 278MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.79G/4.98G [00:17<00:00, 257MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  97% 4.82G/4.98G [00:17<00:00, 262MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.85G/4.98G [00:17<00:00, 263MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.89G/4.98G [00:17<00:00, 268MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.92G/4.98G [00:17<00:00, 270MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.95G/4.98G [00:18<00:00, 272MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors: 100% 4.98G/4.98G [00:18<00:00, 274MB/s]\n",
            "Downloading shards:  25% 1/4 [00:18<00:54, 18.29s/it]\n",
            "model-00002-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 31.5M/5.00G [00:00<00:16, 299MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 62.9M/5.00G [00:00<00:17, 284MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   2% 94.4M/5.00G [00:00<00:17, 278MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 126M/5.00G [00:00<00:17, 276MB/s] \u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 157M/5.00G [00:00<00:17, 276MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 189M/5.00G [00:00<00:17, 274MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 220M/5.00G [00:00<00:17, 275MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   5% 252M/5.00G [00:00<00:17, 276MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   6% 283M/5.00G [00:01<00:21, 222MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   6% 315M/5.00G [00:01<00:20, 229MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   7% 346M/5.00G [00:01<00:19, 238MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 377M/5.00G [00:01<00:18, 248MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 409M/5.00G [00:01<00:18, 254MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 440M/5.00G [00:01<00:19, 237MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 472M/5.00G [00:01<00:18, 250MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  10% 503M/5.00G [00:02<00:20, 223MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  11% 535M/5.00G [00:02<00:19, 223MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  11% 566M/5.00G [00:02<00:19, 230MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  12% 598M/5.00G [00:02<00:18, 241MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 629M/5.00G [00:02<00:17, 251MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 661M/5.00G [00:02<00:17, 255MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 692M/5.00G [00:02<00:16, 262MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 724M/5.00G [00:02<00:16, 267MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  15% 755M/5.00G [00:02<00:15, 272MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 786M/5.00G [00:03<00:15, 278MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 818M/5.00G [00:03<00:15, 277MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  17% 849M/5.00G [00:03<00:14, 278MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 881M/5.00G [00:03<00:14, 280MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 912M/5.00G [00:03<00:14, 280MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  19% 944M/5.00G [00:03<00:14, 283MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 975M/5.00G [00:03<00:14, 283MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 1.01G/5.00G [00:03<00:14, 283MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.04G/5.00G [00:03<00:13, 284MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.07G/5.00G [00:04<00:13, 283MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  22% 1.10G/5.00G [00:04<00:13, 283MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.13G/5.00G [00:04<00:13, 284MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.16G/5.00G [00:04<00:13, 282MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  24% 1.20G/5.00G [00:04<00:13, 283MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.23G/5.00G [00:04<00:13, 280MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.26G/5.00G [00:04<00:13, 280MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.29G/5.00G [00:04<00:13, 275MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.32G/5.00G [00:04<00:13, 272MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  27% 1.35G/5.00G [00:05<00:13, 276MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  28% 1.38G/5.00G [00:05<00:13, 273MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  28% 1.42G/5.00G [00:05<00:13, 271MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  29% 1.45G/5.00G [00:05<00:13, 273MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.48G/5.00G [00:05<00:12, 273MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.51G/5.00G [00:05<00:12, 276MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.54G/5.00G [00:05<00:12, 269MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.57G/5.00G [00:05<00:12, 270MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  32% 1.60G/5.00G [00:06<00:12, 272MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.64G/5.00G [00:06<00:12, 273MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.67G/5.00G [00:06<00:12, 271MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  34% 1.70G/5.00G [00:06<00:12, 272MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.73G/5.00G [00:06<00:12, 270MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.76G/5.00G [00:06<00:11, 271MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.79G/5.00G [00:06<00:11, 269MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.82G/5.00G [00:06<00:11, 269MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  37% 1.86G/5.00G [00:06<00:11, 269MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.89G/5.00G [00:07<00:11, 272MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.92G/5.00G [00:07<00:11, 273MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  39% 1.95G/5.00G [00:07<00:11, 272MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 1.98G/5.00G [00:07<00:11, 271MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 2.01G/5.00G [00:07<00:10, 273MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  41% 2.04G/5.00G [00:07<00:10, 273MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.08G/5.00G [00:07<00:10, 274MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.11G/5.00G [00:08<00:16, 174MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.14G/5.00G [00:08<00:14, 193MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.17G/5.00G [00:08<00:13, 211MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  44% 2.20G/5.00G [00:08<00:12, 227MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  45% 2.23G/5.00G [00:08<00:11, 241MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  45% 2.26G/5.00G [00:08<00:11, 247MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  46% 2.30G/5.00G [00:08<00:10, 254MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.33G/5.00G [00:08<00:10, 260MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.36G/5.00G [00:09<00:09, 270MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.39G/5.00G [00:09<00:09, 275MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.42G/5.00G [00:09<00:09, 277MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  49% 2.45G/5.00G [00:09<00:09, 280MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  50% 2.49G/5.00G [00:09<00:08, 281MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  50% 2.52G/5.00G [00:09<00:08, 279MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  51% 2.55G/5.00G [00:09<00:08, 282MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.58G/5.00G [00:09<00:08, 279MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.61G/5.00G [00:09<00:08, 280MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.64G/5.00G [00:10<00:08, 279MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.67G/5.00G [00:10<00:08, 277MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  54% 2.71G/5.00G [00:10<00:08, 281MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.74G/5.00G [00:10<00:08, 282MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.77G/5.00G [00:10<00:07, 282MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  56% 2.80G/5.00G [00:10<00:07, 285MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.83G/5.00G [00:10<00:07, 283MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.86G/5.00G [00:10<00:07, 284MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  58% 2.89G/5.00G [00:10<00:07, 285MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.93G/5.00G [00:11<00:07, 284MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.96G/5.00G [00:11<00:07, 286MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 2.99G/5.00G [00:11<00:07, 285MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 3.02G/5.00G [00:11<00:07, 281MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  61% 3.05G/5.00G [00:11<00:06, 279MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.08G/5.00G [00:11<00:06, 278MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.11G/5.00G [00:11<00:06, 280MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  63% 3.15G/5.00G [00:11<00:06, 282MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.18G/5.00G [00:11<00:06, 283MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.21G/5.00G [00:12<00:06, 281MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.24G/5.00G [00:12<00:06, 275MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.27G/5.00G [00:12<00:06, 277MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  66% 3.30G/5.00G [00:12<00:06, 277MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  67% 3.33G/5.00G [00:12<00:06, 274MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  67% 3.37G/5.00G [00:12<00:05, 275MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  68% 3.40G/5.00G [00:12<00:05, 272MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.43G/5.00G [00:12<00:05, 272MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.46G/5.00G [00:12<00:05, 271MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.49G/5.00G [00:13<00:06, 230MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.52G/5.00G [00:13<00:06, 243MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  71% 3.55G/5.00G [00:13<00:05, 255MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.59G/5.00G [00:13<00:05, 263MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.62G/5.00G [00:13<00:05, 270MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  73% 3.65G/5.00G [00:13<00:04, 271MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.68G/5.00G [00:13<00:04, 271MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.71G/5.00G [00:13<00:04, 272MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  75% 3.74G/5.00G [00:14<00:04, 275MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.77G/5.00G [00:14<00:04, 275MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.81G/5.00G [00:14<00:04, 271MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.84G/5.00G [00:14<00:04, 272MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.87G/5.00G [00:14<00:04, 275MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  78% 3.90G/5.00G [00:14<00:04, 272MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.93G/5.00G [00:14<00:03, 271MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.96G/5.00G [00:14<00:03, 266MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  80% 4.00G/5.00G [00:14<00:03, 265MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.03G/5.00G [00:15<00:03, 264MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.06G/5.00G [00:15<00:03, 267MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.09G/5.00G [00:15<00:03, 270MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.12G/5.00G [00:15<00:03, 274MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  83% 4.15G/5.00G [00:15<00:03, 274MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  84% 4.18G/5.00G [00:15<00:02, 273MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  84% 4.22G/5.00G [00:15<00:02, 274MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  85% 4.25G/5.00G [00:15<00:02, 274MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.28G/5.00G [00:16<00:02, 276MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.31G/5.00G [00:16<00:02, 276MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.34G/5.00G [00:16<00:02, 275MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.37G/5.00G [00:16<00:02, 276MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  88% 4.40G/5.00G [00:16<00:02, 277MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.44G/5.00G [00:16<00:02, 276MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.47G/5.00G [00:16<00:01, 275MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  90% 4.50G/5.00G [00:16<00:01, 277MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.53G/5.00G [00:16<00:01, 279MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.56G/5.00G [00:17<00:01, 279MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.59G/5.00G [00:17<00:01, 281MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.62G/5.00G [00:17<00:01, 281MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  93% 4.66G/5.00G [00:17<00:01, 281MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.69G/5.00G [00:17<00:01, 282MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.72G/5.00G [00:17<00:00, 282MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  95% 4.75G/5.00G [00:17<00:00, 283MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.78G/5.00G [00:17<00:00, 270MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.81G/5.00G [00:17<00:00, 274MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  97% 4.84G/5.00G [00:18<00:00, 276MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.88G/5.00G [00:18<00:00, 277MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.91G/5.00G [00:18<00:00, 276MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.94G/5.00G [00:18<00:00, 277MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.97G/5.00G [00:18<00:00, 277MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors: 100% 5.00G/5.00G [00:18<00:00, 268MB/s]\n",
            "Downloading shards:  50% 2/4 [00:37<00:37, 18.58s/it]\n",
            "model-00003-of-00004.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   1% 31.5M/4.92G [00:00<00:16, 294MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   1% 62.9M/4.92G [00:00<00:16, 286MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   2% 94.4M/4.92G [00:00<00:17, 283MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   3% 126M/4.92G [00:00<00:17, 280MB/s] \u001b[A\n",
            "model-00003-of-00004.safetensors:   3% 157M/4.92G [00:00<00:16, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   4% 189M/4.92G [00:00<00:16, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   4% 220M/4.92G [00:00<00:16, 282MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   5% 252M/4.92G [00:00<00:16, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   6% 283M/4.92G [00:01<00:16, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   6% 315M/4.92G [00:01<00:16, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   7% 346M/4.92G [00:01<00:16, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   8% 377M/4.92G [00:01<00:16, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   8% 409M/4.92G [00:01<00:16, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   9% 440M/4.92G [00:01<00:16, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  10% 472M/4.92G [00:01<00:15, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  10% 503M/4.92G [00:01<00:15, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  11% 535M/4.92G [00:01<00:15, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  12% 566M/4.92G [00:02<00:15, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  12% 598M/4.92G [00:02<00:15, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  13% 629M/4.92G [00:02<00:15, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  13% 661M/4.92G [00:02<00:15, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  14% 692M/4.92G [00:02<00:15, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  15% 724M/4.92G [00:02<00:15, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  15% 755M/4.92G [00:02<00:15, 275MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  16% 786M/4.92G [00:02<00:15, 272MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  17% 818M/4.92G [00:02<00:15, 273MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  17% 849M/4.92G [00:03<00:14, 274MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  18% 881M/4.92G [00:03<00:14, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  19% 912M/4.92G [00:03<00:14, 272MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  19% 944M/4.92G [00:03<00:14, 273MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  20% 975M/4.92G [00:03<00:14, 273MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  20% 1.01G/4.92G [00:03<00:14, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  21% 1.04G/4.92G [00:03<00:22, 175MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  22% 1.07G/4.92G [00:04<00:19, 197MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  22% 1.10G/4.92G [00:04<00:17, 216MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  23% 1.13G/4.92G [00:04<00:16, 233MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  24% 1.16G/4.92G [00:04<00:15, 243MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  24% 1.20G/4.92G [00:04<00:14, 251MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  25% 1.23G/4.92G [00:04<00:14, 256MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  26% 1.26G/4.92G [00:04<00:13, 264MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  26% 1.29G/4.92G [00:04<00:13, 264MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  27% 1.32G/4.92G [00:04<00:13, 267MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  28% 1.35G/4.92G [00:05<00:13, 271MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  28% 1.38G/4.92G [00:05<00:12, 273MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  29% 1.42G/4.92G [00:05<00:12, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  29% 1.45G/4.92G [00:05<00:12, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  30% 1.48G/4.92G [00:05<00:12, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  31% 1.51G/4.92G [00:05<00:12, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  31% 1.54G/4.92G [00:05<00:12, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  32% 1.57G/4.92G [00:05<00:11, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  33% 1.60G/4.92G [00:05<00:11, 282MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  33% 1.64G/4.92G [00:06<00:11, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  34% 1.67G/4.92G [00:06<00:11, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  35% 1.70G/4.92G [00:06<00:11, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  35% 1.73G/4.92G [00:06<00:11, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  36% 1.76G/4.92G [00:06<00:11, 274MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  36% 1.79G/4.92G [00:06<00:11, 274MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  37% 1.82G/4.92G [00:06<00:11, 273MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  38% 1.86G/4.92G [00:06<00:11, 274MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  38% 1.89G/4.92G [00:07<00:10, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  39% 1.92G/4.92G [00:07<00:10, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  40% 1.95G/4.92G [00:07<00:10, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  40% 1.98G/4.92G [00:07<00:10, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  41% 2.01G/4.92G [00:07<00:10, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  42% 2.04G/4.92G [00:07<00:10, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  42% 2.08G/4.92G [00:07<00:10, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  43% 2.11G/4.92G [00:07<00:10, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  44% 2.14G/4.92G [00:07<00:10, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  44% 2.17G/4.92G [00:08<00:09, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  45% 2.20G/4.92G [00:08<00:09, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  45% 2.23G/4.92G [00:08<00:09, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  46% 2.26G/4.92G [00:08<00:09, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  47% 2.30G/4.92G [00:08<00:09, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  47% 2.33G/4.92G [00:08<00:09, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  48% 2.36G/4.92G [00:08<00:09, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  49% 2.39G/4.92G [00:08<00:09, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  49% 2.42G/4.92G [00:08<00:08, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  50% 2.45G/4.92G [00:09<00:08, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  51% 2.49G/4.92G [00:09<00:08, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  51% 2.52G/4.92G [00:09<00:08, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  52% 2.55G/4.92G [00:09<00:08, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  52% 2.58G/4.92G [00:09<00:08, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  53% 2.61G/4.92G [00:09<00:08, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  54% 2.64G/4.92G [00:09<00:08, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  54% 2.67G/4.92G [00:09<00:07, 284MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  55% 2.71G/4.92G [00:09<00:07, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  56% 2.74G/4.92G [00:10<00:07, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  56% 2.77G/4.92G [00:10<00:07, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  57% 2.80G/4.92G [00:10<00:07, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  58% 2.83G/4.92G [00:10<00:07, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  58% 2.86G/4.92G [00:10<00:07, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  59% 2.89G/4.92G [00:10<00:07, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  60% 2.93G/4.92G [00:10<00:07, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  60% 2.96G/4.92G [00:10<00:06, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 2.99G/4.92G [00:10<00:06, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 3.02G/4.92G [00:11<00:06, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 3.05G/4.92G [00:11<00:06, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  63% 3.08G/4.92G [00:11<00:06, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  63% 3.11G/4.92G [00:11<00:06, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  64% 3.15G/4.92G [00:11<00:06, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  65% 3.18G/4.92G [00:11<00:06, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  65% 3.21G/4.92G [00:11<00:06, 277MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  66% 3.24G/4.92G [00:11<00:06, 274MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  67% 3.27G/4.92G [00:12<00:06, 274MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  67% 3.30G/4.92G [00:12<00:05, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  68% 3.33G/4.92G [00:12<00:05, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  68% 3.37G/4.92G [00:12<00:05, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  69% 3.40G/4.92G [00:12<00:05, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  70% 3.43G/4.92G [00:12<00:05, 282MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  70% 3.46G/4.92G [00:12<00:05, 283MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  71% 3.49G/4.92G [00:12<00:05, 284MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  72% 3.52G/4.92G [00:12<00:04, 284MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  72% 3.55G/4.92G [00:12<00:04, 285MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  73% 3.59G/4.92G [00:13<00:04, 282MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  74% 3.62G/4.92G [00:13<00:04, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  74% 3.65G/4.92G [00:13<00:04, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  75% 3.68G/4.92G [00:13<00:05, 216MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  76% 3.71G/4.92G [00:13<00:05, 229MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  76% 3.74G/4.92G [00:13<00:04, 243MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  77% 3.77G/4.92G [00:13<00:04, 252MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  77% 3.81G/4.92G [00:14<00:04, 259MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  78% 3.84G/4.92G [00:14<00:04, 263MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  79% 3.87G/4.92G [00:14<00:03, 270MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  79% 3.90G/4.92G [00:14<00:03, 274MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  80% 3.93G/4.92G [00:14<00:03, 278MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  81% 3.96G/4.92G [00:14<00:03, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  81% 4.00G/4.92G [00:14<00:03, 276MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  82% 4.03G/4.92G [00:14<00:03, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  83% 4.06G/4.92G [00:14<00:03, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  83% 4.09G/4.92G [00:15<00:02, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  84% 4.12G/4.92G [00:15<00:02, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  84% 4.15G/4.92G [00:15<00:02, 282MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  85% 4.18G/4.92G [00:15<00:02, 286MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  86% 4.22G/4.92G [00:15<00:02, 288MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  86% 4.25G/4.92G [00:15<00:02, 286MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  87% 4.28G/4.92G [00:15<00:02, 287MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  88% 4.31G/4.92G [00:15<00:02, 284MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  88% 4.34G/4.92G [00:15<00:02, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  89% 4.37G/4.92G [00:16<00:01, 279MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  90% 4.40G/4.92G [00:16<00:01, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  90% 4.44G/4.92G [00:16<00:01, 283MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  91% 4.47G/4.92G [00:16<00:01, 283MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  92% 4.50G/4.92G [00:16<00:01, 282MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  92% 4.53G/4.92G [00:16<00:01, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  93% 4.56G/4.92G [00:16<00:01, 282MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  93% 4.59G/4.92G [00:16<00:01, 284MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  94% 4.62G/4.92G [00:16<00:01, 284MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  95% 4.66G/4.92G [00:17<00:00, 284MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  95% 4.69G/4.92G [00:17<00:00, 283MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  96% 4.72G/4.92G [00:17<00:00, 285MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  97% 4.75G/4.92G [00:17<00:00, 284MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  97% 4.78G/4.92G [00:17<00:00, 280MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  98% 4.81G/4.92G [00:17<00:00, 282MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  99% 4.84G/4.92G [00:17<00:00, 281MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  99% 4.88G/4.92G [00:17<00:00, 285MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors: 100% 4.92G/4.92G [00:17<00:00, 274MB/s]\n",
            "Downloading shards:  75% 3/4 [00:55<00:18, 18.34s/it]\n",
            "model-00004-of-00004.safetensors:   0% 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   3% 31.5M/1.17G [00:00<00:03, 298MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   5% 62.9M/1.17G [00:00<00:03, 282MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   8% 94.4M/1.17G [00:00<00:03, 281MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  11% 126M/1.17G [00:00<00:03, 277MB/s] \u001b[A\n",
            "model-00004-of-00004.safetensors:  13% 157M/1.17G [00:00<00:03, 278MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  16% 189M/1.17G [00:00<00:03, 278MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  19% 220M/1.17G [00:00<00:03, 279MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  22% 252M/1.17G [00:00<00:03, 280MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  24% 283M/1.17G [00:01<00:03, 279MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  27% 315M/1.17G [00:01<00:03, 280MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  30% 346M/1.17G [00:01<00:02, 276MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  32% 377M/1.17G [00:01<00:02, 275MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  35% 409M/1.17G [00:01<00:02, 275MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  38% 440M/1.17G [00:01<00:02, 277MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  40% 472M/1.17G [00:01<00:02, 279MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  43% 503M/1.17G [00:01<00:02, 276MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  46% 535M/1.17G [00:01<00:02, 277MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  48% 566M/1.17G [00:02<00:02, 277MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  51% 598M/1.17G [00:02<00:02, 267MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  54% 629M/1.17G [00:02<00:01, 270MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  57% 661M/1.17G [00:02<00:01, 270MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  59% 692M/1.17G [00:02<00:01, 274MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  62% 724M/1.17G [00:02<00:01, 276MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  65% 755M/1.17G [00:02<00:01, 277MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  67% 786M/1.17G [00:02<00:01, 279MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  70% 818M/1.17G [00:02<00:01, 278MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  73% 849M/1.17G [00:03<00:01, 277MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  75% 881M/1.17G [00:03<00:01, 278MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  78% 912M/1.17G [00:03<00:00, 278MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  81% 944M/1.17G [00:03<00:00, 279MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  83% 975M/1.17G [00:03<00:00, 252MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  86% 1.01G/1.17G [00:03<00:00, 259MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  89% 1.04G/1.17G [00:03<00:00, 263MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  92% 1.07G/1.17G [00:03<00:00, 264MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  94% 1.10G/1.17G [00:04<00:00, 269MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  97% 1.13G/1.17G [00:04<00:00, 228MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors: 100% 1.17G/1.17G [00:04<00:00, 269MB/s]\n",
            "Downloading shards: 100% 4/4 [00:59<00:00, 14.91s/it]\n",
            "[INFO|modeling_utils.py:1569] 2024-08-14 00:05:00,586 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1038] 2024-08-14 00:05:00,587 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:25<00:00,  6.27s/it]\n",
            "[INFO|modeling_utils.py:4450] 2024-08-14 00:05:27,793 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4458] 2024-08-14 00:05:27,793 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 184/184 [00:00<00:00, 1.69MB/s]\n",
            "[INFO|configuration_utils.py:993] 2024-08-14 00:05:27,968 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/8c22764a7e3675c50d4c7c9a4edb474456022b16/generation_config.json\n",
            "[INFO|configuration_utils.py:1038] 2024-08-14 00:05:27,968 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "08/14/2024 00:08:17 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "08/14/2024 00:08:17 - INFO - llamafactory.model.loader - all params: 8,030,261,248\n",
            "[INFO|trainer.py:3819] 2024-08-14 00:08:18,272 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3821] 2024-08-14 00:08:18,272 >>   Num examples = 400\n",
            "[INFO|trainer.py:3824] 2024-08-14 00:08:18,272 >>   Batch size = 1\n",
            "[WARNING|logging.py:328] 2024-08-14 00:08:29,004 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "100% 400/400 [46:01<00:00,  6.49s/it]Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.721 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 400/400 [46:04<00:00,  6.91s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4                 =    34.9839\n",
            "  predict_model_preparation_time =     0.0016\n",
            "  predict_rouge-1                =    56.1936\n",
            "  predict_rouge-2                =    33.1578\n",
            "  predict_rouge-l                =    31.0168\n",
            "  predict_runtime                = 0:46:15.58\n",
            "  predict_samples_per_second     =      0.144\n",
            "  predict_steps_per_second       =      0.144\n",
            "08/14/2024 00:54:33 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves/LLaMA3.1-8B-Chat/lora/eval_2024-08-14-00-03-12/generated_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      },
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS0Qk5OR0i4Q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"identity,alpaca_en_demo\",             # use alpaca and identity datasets\n",
        "  template=\"llama3\",                     # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,               # the batch size\n",
        "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
        "  logging_steps=10,                      # log every 10 steps\n",
        "  warmup_ratio=0.1,                      # use warmup scheduler\n",
        "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                     # the learning rate\n",
        "  num_train_epochs=3.0,                    # the epochs of training\n",
        "  max_samples=500,                      # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
        "  quantization_bit=4,                     # use 4-bit QLoRA\n",
        "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                         # use float16 mixed precision training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVNaC-xS5N40"
      },
      "source": [
        "## Infer the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "outputs": [],
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  quantization_bit=4,                    # load 4-bit quantized model\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTESHaFvbNTr"
      },
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcNcHcA4bf4Z"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMojogHbaOZF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",              # the path to save the merged model\n",
        "  export_size=2,                       # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "  #export_hub_model_id=\"your_id/your_model\",         # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TeYs5Lz-QJYk"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}